<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>BuyiXiao&#39;s Blog</title>
  <icon>https://www.gravatar.com/avatar/d8d183d1caef8e675edc72dbd213a438</icon>
  
  <link href="https://buyixiao.github.io/atom.xml" rel="self"/>
  
  <link href="https://buyixiao.github.io/"/>
  <updated>2023-02-04T09:11:14.079Z</updated>
  <id>https://buyixiao.github.io/</id>
  
  <author>
    <name>BuyiXiao</name>
    <email>2391527690@qq.com</email>
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>塞缪尔·厄尔曼：青春</title>
    <link href="https://buyixiao.github.io/blog/youth-by-samuel-ullman.html"/>
    <id>https://buyixiao.github.io/blog/youth-by-samuel-ullman.html</id>
    <published>2023-02-04T08:56:14.000Z</published>
    <updated>2023-02-04T09:11:14.079Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>《青春》作者是德裔美国作家塞缪尔·厄尔曼。</p></blockquote><h2 id="中文译文"><a href="#中文译文" class="headerlink" title="中文译文"></a>中文译文</h2><p>青春不是年华，而是心境；青春不是桃面、丹唇、柔膝，而是深沉的意志、恢宏的想象、炽热的感情；青春是生命的深泉在涌流。</p><p>青春气贯长虹，勇锐盖过怯弱，进取压倒苟安。如此锐气，二十后生有之，六旬男子则更多见。年岁有加，并非垂老；理想丢弃，方堕暮年。岁月悠悠，衰微只及肌肤；热忱抛却，颓唐必致灵魂。忧烦，惶恐，丧失自信，定使心灵扭曲，意气如灰。</p><p>无论年届花甲，抑或二八芳龄，心中皆有生命之欢乐，奇迹之诱惑，孩童般天真久盛不衰。</p><p>人人心中皆有一台天线，只要你从天上人间接受美好、希望、欢乐、勇气和力量的信号，你无不青春永驻、风华长存。</p><p>一旦天线降下，锐气便被冰雪覆盖，玩世不恭、自暴自弃油然而生，即便年方二十，实已垂垂老矣；然则只要竖起天线，捕捉乐观信号，你就有望在八十高龄告别尘寰时仍觉年轻。</p><h2 id="英文原文"><a href="#英文原文" class="headerlink" title="英文原文"></a>英文原文</h2><p>《YOUTH 》 by Samuel Ullman</p><p>Youth is not a time of life; it is a state of mind; it is not a matter of rosy cheeks, red lips and supple knees; it is a matter of the will, a quality of the imagination, a vigor of the emotions; it is the freshness of the deep springs of life.</p><p>Youth means a temperamental predominance of courage over timidity of the appetite, for adventure over the love of ease. This often exists in a man of sixty more than a boy of twenty. Nobody grows old merely by a number of years. We grow old by deserting our ideals.</p><p>Years may wrinkle the skin, but to give up enthusiasm wrinkles the soul. Worry, fear, self-distrust bows the heart and turns the spirit back to dust.</p><p>Whether sixty or sixteen, there is in every human being’s heart the lure of wonder, the unfailing child-like appetite of what’s next, and the joy of the game of living. In the center of your heart and my heart there is a wireless station; so long as it receives messages of beauty, hope, cheer, courage and power from men and from the infinite, so long are you young.</p><p>When the aerials are down, and your spirit is covered with snows of cynicism and the ice of pessimism, then you are grown old, even at twenty, but as long as your aerials are up, to catch the waves of optimism, there is hope you may die young at eighty.</p>]]></content>
    
    
    <summary type="html">青春气贯长虹，勇锐盖过怯弱，进取压倒苟安。</summary>
    
    
    
    <category term="励志" scheme="https://buyixiao.github.io/categories/%E5%8A%B1%E5%BF%97/"/>
    
    
    <category term="青春" scheme="https://buyixiao.github.io/tags/%E9%9D%92%E6%98%A5/"/>
    
  </entry>
  
  <entry>
    <title>crontab 定时执行 Python 脚本踩坑记录</title>
    <link href="https://buyixiao.github.io/blog/crontab-python.html"/>
    <id>https://buyixiao.github.io/blog/crontab-python.html</id>
    <published>2023-02-04T07:28:38.000Z</published>
    <updated>2023-02-04T08:48:57.612Z</updated>
    
    <content type="html"><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>换过几个云服务器，每次都要在 crontab 这折腾一会儿，这次索性把问题记录下来，力求详尽。</p><p>笔者的云服务器：腾讯云 2C4G；</p><p>Linux 系统： Ubuntu 20.04 LTS 64bit。</p><blockquote><p>crontab 是 Linux 下周期性执行的指令，常常在后台运行，每一分钟检查是否有预定的作业需要执行。这类作业一般称为 cron jobs。(摘自百度百科)</p></blockquote><h3 id="必备知识"><a href="#必备知识" class="headerlink" title="必备知识"></a>必备知识</h3><p>1、cron 的配置文件可在三个地方存放</p><ul><li><code>/var/spool/cron/crontabs/root</code></li><li><code>/etc/crontab</code></li><li><code>/etc/cron.d/</code></li></ul><p>​      一般情况下，通过 crontab -e 命令编辑的是第一个路径下的配置文件，在这里的命令不需要指定用户为 root；后两个则需要，比如命令 <code> 0 3 * * 1 root python test.py</code>，其中的 root 不可少。</p><p>需要注意的是，如果使用 crontab -e 编辑，修改后使用 Ctrl+X，提示：<code>save modified buffer ...?</code>    ，选择 ：yes，又提示：<code>file name to write</code> ，选择：Ctrl+T，在最后一个界面使用左右箭头切换至 crontab。</p><p>2、虽然说编辑完 crontab 文件后不需要重启 cron 服务，但是包括重启在内的一些命令最好还是了解下。</p><ul><li>重启，各种资料都说是 service crond restart，在笔者的环境上实测是 service cron restart；在 centos 上是 systemctl restart crond，笔者暂未考证。</li><li>状态，笔者亲测为 service cron start；其他环境同上。</li></ul><h3 id="问题记录"><a href="#问题记录" class="headerlink" title="问题记录"></a>问题记录</h3><p>使用命令定时执行 python 脚本，每个小时的第 15 分钟运行一次，无任何反应。命令如下：</p><p><code>15 * * * * /mypath/venv/bin/python3 /mypath/monitor.py &gt;&gt; /mypath/execute.log 2&gt;&amp;1</code></p><p>python 脚本输出的 execute.log 亦无输出。</p><h3 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h3><p>第一步想着查看 crontab 的日志，才知道默认是不打开的需要手动配置。命令如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/rsyslog.d/50-default.conf</span><br><span class="line">cron.*  /var/log/cron.log #将cron前面的注释符去掉</span><br><span class="line"><span class="meta">#</span><span class="bash">重启rsyslog</span></span><br><span class="line">sudo service rsyslog restart</span><br><span class="line">sudo service cron restart</span><br></pre></td></tr></table></figure><p>然后 <code>vi /var/log/cron.log</code> 查看日志，能够发现脚本确实运行了，除此之外没有任何有用信息，看其他博主（参考文末链接2）说是需要安装 postfix，正安装着不知道怎么配置邮件服务器的域名，又看到如果 python 脚本输出配置了重定向日志，不用配这个也行。遂作罢。</p><p>排除了 crontab 的问题，那只有是 python 脚本的问题了，偶然间发现（参考文末链接 3）：python 脚本中涉及到读写文件的动作，一般定时任务都不会执行.；脚本在执行时，由于是通过 crontab 去执行的，它的执行目录会变成当前用户的根目录，如果是root，就会在/root/下执行。</p><p>但是我们读写的文件路径在 root 下吗，大概率不是，一种解决办法是将 python 脚本中的文件路径全部换成服务器绝对路径，但是这样可移植性差；更好的办法是使用 shell 脚本，shell 脚本第一行使用命令 cd 到我们的目的路径，然后第二行修改我们原来的命令（py 脚本的绝对路径也可以简化成相对路径），如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">！/usr/bin/bash</span></span><br><span class="line">cd /mypath</span><br><span class="line">/mypath/venv/bin/python3 /monitor.py </span><br></pre></td></tr></table></figure><p>使用 <code>chmod a+x test.sh</code> 赋予执行权限，然后在 crontab 配置定时运行这个 shell 脚本，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">15 * * * * /bin/sh /mypath/test.sh &gt;&gt; /mypath/execute.log 2&gt;&amp;1</span><br></pre></td></tr></table></figure><p>最后可能还有一个坑，查看 execute.log，发现无法 cd 到 mypath，这是因为这个 test.sh 是通过 rz 上传的，不是在服务器上通过 touch 创建的，无法识别，解决办法就在原因中，touch 创建再复制命令就行。</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>1、<a href="https://blog.tankywoo.com/2014/10/05/the-holes-of-crontab.html">一个 crontab 的坑</a></p><p>2、<a href="https://my.oschina.net/leejun2005/blog/1788342">迷之 crontab 异常：不运行、不报错、无日志</a></p><p>3、<a href="https://blog.csdn.net/xys2333/article/details/112469461">crontab运行python脚本不生效问题</a></p>]]></content>
    
    
    <summary type="html">根治 crontab 运行 python 脚本的疑难杂症</summary>
    
    
    
    <category term="Linux" scheme="https://buyixiao.github.io/categories/Linux/"/>
    
    
    <category term="crontab" scheme="https://buyixiao.github.io/tags/crontab/"/>
    
    <category term="python" scheme="https://buyixiao.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>pandas groupby filter 函数妙用</title>
    <link href="https://buyixiao.github.io/blog/pandas-groupby-filter.html"/>
    <id>https://buyixiao.github.io/blog/pandas-groupby-filter.html</id>
    <published>2023-01-07T02:26:53.000Z</published>
    <updated>2023-01-08T07:00:14.783Z</updated>
    
    <content type="html"><![CDATA[<p>假设有一个狂人日记的 dataframe 如下：</p><table><thead><tr><th align="center">user_name</th><th align="center">publish_time</th><th align="center">content</th></tr></thead><tbody><tr><td align="center">小明</td><td align="center">2022-12-30 15:10:00</td><td align="center">今天是 2022 年最后一天，我在广东</td></tr><tr><td align="center">小刚</td><td align="center">2022-01-01 12:23:33</td><td align="center">今天是 2022 年第一天，我在加勒比</td></tr><tr><td align="center">小王</td><td align="center">2022-01-01 12:33:00</td><td align="center">今天是 2022 年第一天，我在小刚身边</td></tr><tr><td align="center">小刚</td><td align="center">2023-01-01 02:15:45</td><td align="center">今天是 2023 年第一天，我在百慕大</td></tr><tr><td align="center">小明</td><td align="center">2023-01-01 00:05:20</td><td align="center">今天是 2023 年第一天，我还在广东</td></tr></tbody></table><p>现在我们要统计狂人日记里面，同一作者第一次和最后一次发布时间差大于 30 天的行。</p><p>乍一看，首先必须统计同一作者至少发布两次的行，也就是上一篇<a href="https://buyixiao.github.io/blog/pandas-value-counts.html">value counts</a> 的内容。</p><p>然后再使用 groupby 分组 + filter 过滤实现，这个 filter 相当于 mysql 语句中 groupby 后的 having 语句，是在分组上做筛选的。</p><p>所以在上一篇的基础上，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># author:           inspurer(月小水长)</span></span><br><span class="line"><span class="comment"># create_time:      2023/1/7 8:58</span></span><br><span class="line"><span class="comment"># 运行环境           Python3.6+</span></span><br><span class="line"><span class="comment"># github            https://github.com/inspurer</span></span><br><span class="line"><span class="comment"># website           https://buyixiao.github.io/</span></span><br><span class="line"><span class="comment"># 微信公众号         月小水长</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">input_file = <span class="string">&#x27;./狂人日记 2022.csv&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove_show_count_below_n</span>(<span class="params">input_file, col, n=<span class="number">2</span></span>):</span></span><br><span class="line">    df = pd.read_csv(input_file)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一步，筛选 user_name 出现大于等于 2 次的行</span></span><br><span class="line">    count_df = pd.DataFrame(df[col].value_counts())</span><br><span class="line">    count_df.columns = [<span class="string">&#x27;count&#x27;</span>]</span><br><span class="line">    not_below_n_index = count_df[count_df[<span class="string">&#x27;count&#x27;</span>] &gt;= n].index</span><br><span class="line">    df = df[df[col].isin(not_below_n_index)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 比上一篇新加的內容</span></span><br><span class="line">    df[<span class="string">&#x27;publish_date&#x27;</span>] = pd.to_datetime(df[<span class="string">&#x27;publish_time&#x27;</span>]).dt.date</span><br><span class="line">    df = df.groupby(col).<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: (x[<span class="string">&#x27;publish_date&#x27;</span>].<span class="built_in">max</span>() - x[<span class="string">&#x27;publish_date&#x27;</span>].<span class="built_in">min</span>()).days &gt; <span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">    df.to_csv(<span class="string">&#x27;result.csv&#x27;</span>, index=<span class="literal">False</span>, encoding=<span class="string">&#x27;utf-8-sig&#x27;</span>)</span><br><span class="line"></span><br><span class="line">remove_show_count_below_n(input_file, col=<span class="string">&#x27;user_name&#x27;</span>)</span><br></pre></td></tr></table></figure><p>仔细想想，第一步真的有必要吗？如果只出现一次，在第二步 publish_date 相减的时候差值为 0，直接就过滤掉了，所以第一步在这个任务是多此一举。简化代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">time_interval_filter</span>(<span class="params">input_file, col, days</span>):</span></span><br><span class="line">    df = pd.read_csv(input_file)</span><br><span class="line">    df[<span class="string">&#x27;publish_date&#x27;</span>] = pd.to_datetime(df[<span class="string">&#x27;publish_time&#x27;</span>]).dt.date</span><br><span class="line">    df = df.groupby(col).<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: (x[<span class="string">&#x27;publish_date&#x27;</span>].<span class="built_in">max</span>() - x[<span class="string">&#x27;publish_date&#x27;</span>].<span class="built_in">min</span>()).days &gt; days)</span><br><span class="line">    df.to_csv(<span class="string">&#x27;result.csv&#x27;</span>, index=<span class="literal">False</span>, encoding=<span class="string">&#x27;utf-8-sig&#x27;</span>)</span><br><span class="line"></span><br><span class="line">time_interval_filter(input_file, col=<span class="string">&#x27;user_name&#x27;</span>, days=<span class="number">30</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">筛选出两次时间差大于任意时间的内容</summary>
    
    
    
    <category term="Python" scheme="https://buyixiao.github.io/categories/Python/"/>
    
    
    <category term="filter" scheme="https://buyixiao.github.io/tags/filter/"/>
    
    <category term="groupby" scheme="https://buyixiao.github.io/tags/groupby/"/>
    
    <category term="pandas" scheme="https://buyixiao.github.io/tags/pandas/"/>
    
  </entry>
  
  <entry>
    <title>pandas 筛选某列值出现至少 N 次的行</title>
    <link href="https://buyixiao.github.io/blog/pandas-value-counts.html"/>
    <id>https://buyixiao.github.io/blog/pandas-value-counts.html</id>
    <published>2023-01-07T01:59:51.000Z</published>
    <updated>2023-01-08T06:59:02.854Z</updated>
    
    <content type="html"><![CDATA[<p>假设有一个 dataframe 如下：</p><table><thead><tr><th align="center">user_name</th><th align="center">publish_time</th><th align="center">content</th></tr></thead><tbody><tr><td align="center">小明</td><td align="center">2022-12-30 15:10:00</td><td align="center">今天是 2022 年最后一天，我在广东</td></tr><tr><td align="center">小刚</td><td align="center">2022-01-01 12:23:33</td><td align="center">今天是 2022 年第一天，我在加勒比</td></tr><tr><td align="center">小王</td><td align="center">2022-01-01 12:33:00</td><td align="center">今天是 2022 年第一天，我在小刚身边</td></tr><tr><td align="center">小刚</td><td align="center">2023-01-01 02:15:45</td><td align="center">今天是 2023 年第一天，我在百慕大</td></tr><tr><td align="center">小明</td><td align="center">2023-01-01 00:05:20</td><td align="center">今天是 2023 年第一天，我还在广东</td></tr></tbody></table><p>现在我们要统计 user_name 中出现两次及以上的行。肉眼可以看出就是小明、小刚各自两行共四行。</p><p>最开始我的思路是使用 drop_duplicated 按照 user_name 为 key 去重，将去重后的 dataframe 和原来的 dataframe 按照所有列为 key 合并后再去重，这样一来就只剩下 user_name 出现两次及以上的行了，但是这种思路扩展性不好，假如是出现  3 次及以上呢？计算量就更大了。</p><p>改进后的思路是：主要使用 pandas 的 value_counts 函数统计次数，isin 函数实现筛选，其代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># author:           inspurer(月小水长)</span></span><br><span class="line"><span class="comment"># create_time:      2023/1/7 8:58</span></span><br><span class="line"><span class="comment"># 运行环境           Python3.6+</span></span><br><span class="line"><span class="comment"># github            https://github.com/inspurer</span></span><br><span class="line"><span class="comment"># website           https://buyixiao.github.io/</span></span><br><span class="line"><span class="comment"># 微信公众号         月小水长</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">input_file = <span class="string">&#x27;./狂人日记 2022.csv&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove_show_count_below_n</span>(<span class="params">input_file, col, n=<span class="number">2</span></span>):</span></span><br><span class="line">    df = pd.read_csv(input_file)</span><br><span class="line"></span><br><span class="line">    count_df = pd.DataFrame(df[col].value_counts())</span><br><span class="line">    count_df.columns = [<span class="string">&#x27;count&#x27;</span>]</span><br><span class="line">    not_below_n_index = count_df[count_df[<span class="string">&#x27;count&#x27;</span>] &gt;= n].index</span><br><span class="line">    df = df[df[col].isin(not_below_n_index)]</span><br><span class="line"></span><br><span class="line">    df.to_csv(<span class="string">&#x27;result.csv&#x27;</span>, index=<span class="literal">False</span>, encoding=<span class="string">&#x27;utf-8-sig&#x27;</span>)</span><br><span class="line"></span><br><span class="line">remove_show_count_below_n(input_file, col=<span class="string">&#x27;user_name&#x27;</span>)</span><br></pre></td></tr></table></figure><p>如有更优雅的方式（肯定有），请批评指正～</p>]]></content>
    
    
    <summary type="html">不用 groupby，使用 value_counts 统计次数</summary>
    
    
    
    <category term="Python" scheme="https://buyixiao.github.io/categories/Python/"/>
    
    
    <category term="pandas" scheme="https://buyixiao.github.io/tags/pandas/"/>
    
    <category term="value_counts" scheme="https://buyixiao.github.io/tags/value-counts/"/>
    
    <category term="frequency" scheme="https://buyixiao.github.io/tags/frequency/"/>
    
  </entry>
  
  <entry>
    <title>android camera2 实战经验汇总</title>
    <link href="https://buyixiao.github.io/blog/android-camera2.html"/>
    <id>https://buyixiao.github.io/blog/android-camera2.html</id>
    <published>2022-06-04T15:07:44.000Z</published>
    <updated>2022-06-04T15:36:38.827Z</updated>
    
    <content type="html"><![CDATA[<h3 id="android-camera2-简单介绍"><a href="#android-camera2-简单介绍" class="headerlink" title="android camera2 简单介绍"></a>android camera2 简单介绍</h3><p>从 Android 5.0 开始，Google 引入了一套全新的相机框架 Camera2 api，它相比较 Camera1 有以下优势：</p><p>1、可以获取更多的帧(预览/拍照)信息以及手动控制每一帧的参数<br>2、对Camera的控制更加完全(比如支持调整focus distance, 剪裁预览/拍照图片)<br>3、支持更多图片格式(yuv/raw)以及高速连拍<br>4、…</p><p>现在是 2022 年了，可以说 99% 以上的安卓手机都在 5.0 系统，因此完全不用担心兼容问题。本文只是记录在自定义 camera2 实现连拍过程的坑，具体有关 camera2 的介绍可以参考 Google 的文档。</p><blockquote><p><a href="https://developer.android.com/training/camera2">https://developer.android.com/training/camera2</a></p></blockquote><h3 id="camera2-实现连拍并保存"><a href="#camera2-实现连拍并保存" class="headerlink" title="camera2 实现连拍并保存"></a>camera2 实现连拍并保存</h3><p>理论上 camera2 连拍可以达到 30fps，笔者需要的速度是 20s 拍 100 张并保存，实测无压力。主要是使用 CountDownTimer 定时 build CaptureRequest，然后在 CaptureResult 中使用 RxJava 线程调度，即在 IO 线程保存照片，在 UI 线程更新。</p><h3 id="遇到的一些问题"><a href="#遇到的一些问题" class="headerlink" title="遇到的一些问题"></a>遇到的一些问题</h3><h4 id="预览正常，保存的照片旋转了-90-度"><a href="#预览正常，保存的照片旋转了-90-度" class="headerlink" title="预览正常，保存的照片旋转了 90 度"></a>预览正常，保存的照片旋转了 90 度</h4><p>解决办法有两个，第一个就在在 CaptureRequest 中构建 bitmap，然后强行使这个 bitmap 旋转到原来的位置。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> Bitmap <span class="title">adjustPhotoRotation</span><span class="params">(Bitmap bm, <span class="keyword">final</span> <span class="keyword">int</span> orientationDegree)</span> </span>&#123;</span><br><span class="line">Matrix m = <span class="keyword">new</span> Matrix();</span><br><span class="line">       m.setRotate(orientationDegree, (<span class="keyword">float</span>) bm.getWidth() / <span class="number">2</span>, (<span class="keyword">float</span>) bm.getHeight() / <span class="number">2</span>);</span><br><span class="line">       <span class="keyword">try</span> &#123;</span><br><span class="line">           <span class="keyword">return</span> Bitmap.createBitmap(bm, <span class="number">0</span>, <span class="number">0</span>, bm.getWidth(), bm.getHeight(), m, <span class="keyword">true</span>);</span><br><span class="line">       &#125; <span class="keyword">catch</span> (OutOfMemoryError ex) &#123;</span><br><span class="line">           ex.fillInStackTrace();</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> bm;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Bitmap bitmapImage = BitmapFactory.decodeByteArray(data, <span class="number">0</span>, data.length, <span class="keyword">null</span>);</span><br><span class="line"><span class="comment">// 这个 90 度太唐突了，但是能解决问题</span></span><br><span class="line">Bitmap newBitmap = adjustPhotoRotation(bitmapImage, <span class="number">90</span>);</span><br></pre></td></tr></table></figure><p>第二个是 requestBuilder 设置 JPEG_ORIENTATION，这种才是解决根本</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getDisplayRotation</span><span class="params">(Activity activity)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (activity == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> rotation = activity.getWindowManager().getDefaultDisplay()</span><br><span class="line">                .getRotation();</span><br><span class="line">        <span class="keyword">switch</span> (rotation) &#123;</span><br><span class="line">            <span class="keyword">case</span> Surface.ROTATION_0:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">90</span>;</span><br><span class="line">            <span class="keyword">case</span> Surface.ROTATION_90:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">case</span> Surface.ROTATION_180:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">270</span>;</span><br><span class="line">            <span class="keyword">case</span> Surface.ROTATION_270:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">180</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 构建 requestBuilder 的时候设置</span></span><br><span class="line">mCaptureReqBuilder.set(CaptureRequest.JPEG_ORIENTATION, getDisplayRotation(activity));</span><br></pre></td></tr></table></figure><h4 id="CaptureRequest-Builder-NPE"><a href="#CaptureRequest-Builder-NPE" class="headerlink" title="CaptureRequest$Builder NPE"></a>CaptureRequest$Builder NPE</h4><p>完整报错信息是：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.lang.NullPointerException: Attempt to invoke virtual method <span class="string">&#x27;android.hardware.camera2.CaptureRequest$Builder android.hardware.camera2.CameraDevice.createCaptureRequest(int)&#x27;</span> on a <span class="keyword">null</span> object reference</span><br></pre></td></tr></table></figure><p>这是因为，相机驱动相关初始化后要延迟 1s 左右才能 buildCaptureRequest，post 一个延时 1s 的 runnable 即可</p><h4 id="部分机型上界面拉伸，保存正常"><a href="#部分机型上界面拉伸，保存正常" class="headerlink" title="部分机型上界面拉伸，保存正常"></a>部分机型上界面拉伸，保存正常</h4><p>试了很多，暂无解，待填</p>]]></content>
    
    
    <summary type="html">最近需要自定义 android camera2 实现连拍，此文为踩坑经验汇总</summary>
    
    
    
    <category term="Android" scheme="https://buyixiao.github.io/categories/Android/"/>
    
    
    <category term="camera2" scheme="https://buyixiao.github.io/tags/camera2/"/>
    
    <category term="android" scheme="https://buyixiao.github.io/tags/android/"/>
    
    <category term="连拍" scheme="https://buyixiao.github.io/tags/%E8%BF%9E%E6%8B%8D/"/>
    
  </entry>
  
  <entry>
    <title>pandas 分组频率统计</title>
    <link href="https://buyixiao.github.io/blog/pandas-groupby-frequency-statistics.html"/>
    <id>https://buyixiao.github.io/blog/pandas-groupby-frequency-statistics.html</id>
    <published>2022-05-28T12:50:25.000Z</published>
    <updated>2022-05-28T13:30:06.461Z</updated>
    
    <content type="html"><![CDATA[<p>假设有一个 dataframe 如下：</p><table><thead><tr><th align="center">country_name</th><th align="center">date</th><th align="center">标题</th></tr></thead><tbody><tr><td align="center">中国</td><td align="center">20030101</td><td align="center">今天是 2003 年第一天，我在中国</td></tr><tr><td align="center">安提瓜和巴布达</td><td align="center">20030101</td><td align="center">今天是 2003 年第一天，我在安提瓜和巴布达</td></tr><tr><td align="center">中国</td><td align="center">20030102</td><td align="center">今天是 2003 年第二天，我在中国</td></tr><tr><td align="center">蒙古</td><td align="center">20030102</td><td align="center">今天是 2003 年第二天，我在蒙古</td></tr></tbody></table><p>现在要统计每天每个国家在当天出现的频率，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># author:           inspurer(月小水长)</span></span><br><span class="line"><span class="comment"># create_time:      2022/5/28 20:10</span></span><br><span class="line"><span class="comment"># 运行环境           Python3.6+</span></span><br><span class="line"><span class="comment"># github            https://github.com/inspurer</span></span><br><span class="line"><span class="comment"># website           https://buyixiao.github.io/</span></span><br><span class="line"><span class="comment"># 微信公众号         月小水长</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">input_file = <span class="string">&#x27;all_country.csv&#x27;</span></span><br><span class="line"></span><br><span class="line">df = pd.read_csv(input_file)</span><br><span class="line"></span><br><span class="line">res_df = df.groupby([<span class="string">&#x27;date&#x27;</span>, <span class="string">&#x27;country_name&#x27;</span>]).count().reset_index()</span><br><span class="line"></span><br><span class="line">res_df = res_df[res_df.columns[:<span class="number">3</span>]]</span><br><span class="line"></span><br><span class="line">res_df.rename(columns=&#123;<span class="string">&#x27;标题&#x27;</span>: <span class="string">&#x27;daily_cnt&#x27;</span>&#125;, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(res_df, res_df.columns)</span><br><span class="line"></span><br><span class="line">res_df[<span class="string">&#x27;daily_frq&#x27;</span>] = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(res_df.shape[<span class="number">0</span>])]</span><br><span class="line"><span class="keyword">for</span> index, row <span class="keyword">in</span> res_df.iterrows():</span><br><span class="line">    res_df.loc[index, <span class="string">&#x27;daily_frq&#x27;</span>] = <span class="built_in">round</span>(row[<span class="string">&#x27;daily_cnt&#x27;</span>] / df[df[<span class="string">&#x27;date&#x27;</span>] == row[<span class="string">&#x27;date&#x27;</span>]].shape[<span class="number">0</span>], <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">res_df.to_csv(<span class="string">&quot;res_&quot;</span> + input_file, index=<span class="literal">False</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>如有更优雅的方式（肯定有），请批评指正~</p>]]></content>
    
    
    <summary type="html">按照 A 列分组，统计 B 列下每个取值的频率</summary>
    
    
    
    <category term="Python" scheme="https://buyixiao.github.io/categories/Python/"/>
    
    
    <category term="groupby" scheme="https://buyixiao.github.io/tags/groupby/"/>
    
    <category term="pandas" scheme="https://buyixiao.github.io/tags/pandas/"/>
    
    <category term="frequency" scheme="https://buyixiao.github.io/tags/frequency/"/>
    
  </entry>
  
  <entry>
    <title>【持续更新|2022最新】68w 高质量新闻数据集</title>
    <link href="https://buyixiao.github.io/blog/qualitative-news-dataset.html"/>
    <id>https://buyixiao.github.io/blog/qualitative-news-dataset.html</id>
    <published>2022-05-14T15:36:29.000Z</published>
    <updated>2022-05-14T15:45:21.989Z</updated>
    
    <content type="html"><![CDATA[<p>租用服务器，累计半年有余对新浪，腾讯，澎湃三个国内主流新闻站点进行抓取，共计保存 68 w 数据，约 1.8G，导出到本地 csv 花了 5 个小时。</p><p>csv 一共 11 列，分别是：新闻抓取时间，标题，来源，头图，发布时间，链接，分类，关键词（逗号分隔），标签，描述，内容。</p><p>数据收集和整理获取花费大量时间和精力，故收取一定费用。下载地址：</p><p><a href="https://afdian.net/p/67bcb002d38f11ecad6152540025c377">https://afdian.net/p/67bcb002d38f11ecad6152540025c377</a></p><p>数据集不定期增量更新到上述下载地址~</p>]]></content>
    
    
    <summary type="html">抓取腾讯、新浪、澎湃新闻，11个字段，1.8G，68w 条</summary>
    
    
    
    <category term="爬虫" scheme="https://buyixiao.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
    <category term="新闻" scheme="https://buyixiao.github.io/tags/%E6%96%B0%E9%97%BB/"/>
    
    <category term="数据集" scheme="https://buyixiao.github.io/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>话『雨』</title>
    <link href="https://buyixiao.github.io/blog/something-about-rain.html"/>
    <id>https://buyixiao.github.io/blog/something-about-rain.html</id>
    <published>2022-04-24T06:08:13.000Z</published>
    <updated>2022-04-24T06:15:42.971Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原载于博主大学期间的一篇空间日志。</p></blockquote><p>今晚自习走出世主楼，不觉已是沉沉的黑夜，还有一点小雨，雨滴滴答答地下，又湿又黑，此情此景，我没有汪国真“寒风冷雨”的感慨，倒是想起一些我和“雨”的那些陈年往事。</p><p>记忆中，我第一次见到对“雨”的文学性描述，大概是一首“大雨淅沥沥，小雨哗啦啦”的儿歌，活泼可爱的曲风，大概奠定了我一直以来对雨的好感。</p><p>一说到下雨，我向来是不喜欢打伞的，原因之一呢，是我太喜欢雨打在脸庞的那种感觉了，虽然意象不同，我这里还是想引用一句“吹面不寒杨柳风”来描绘，那种感觉呐，就像全身心地在和大自然交谈，窃窃私语，不可与人说；还有一个比较现实的原因就是，因为每次打伞，一般都是去参加某种公共活动，比如上学或者聚会，然后玩得欢欣了，一般就会把它遗忘在某个角落，再想起来时它已不在原地等我了，还有一次，那是上小学的时候，有一天放学，阴风怒号，我一个人穿过一段山路回家，在经过一个比较阴沉的水塘的时候，突然风大了起来，把我的伞给吹翻了，然后就飞到水塘里去了，雨很大，水塘很深，而我的手又短，真的是很绝望了。所以每次雨天回家，我妈总是质问我，伞哪去了，后面就懒得问了，只是拿干毛巾擦我的头，怕着凉。当然，如果雨是下的很大，倾盆大雨那种，或者是恰如“一桥清雨一伞开”这种妙不可言可遇不可求的意境，我是很乐意打伞的。 </p><p>下面就要把“雨”和我的吃货属性结合在一起了。 </p><p>梅雨季节，雨就一直下个不停，雨一停，我爸就带上我，以及一个捞鱼的网兜，去那些水渠与河流汇合的地方，用网兜一抄，必有各种野生的鲫鱼鲤鱼鲢鱼…，后来我上了高中，觉得可以用“下雨天新鲜的雨水含氧量太少，而这些汇合处因为和水流动性强结合氧气可能性大”之类的阐述来解释它，也算是学以致用吧。只是不知道为什么，这种时候捕的野生鱼，味道非常鲜美，我爸负责杀鱼，我负责去菜园里采青辣椒和大蒜，然后就是非常幸福的吃货时光了。</p><p> 如果这种时候我爸不带我玩的话，我通常会一个人去后山，它有个确切的名字叫［白鹭山］，说实话我倒没怎么见过白鹭，大小和白鹭相近，颜色和白鹭相似的一种湘南常见的菌类我倒见过不少(学名叫什么我现在也没查到，下次一定要用［形色］去辨一辨)，特别是下雨后，在马尾松的树根处、针丝叶铺得很厚的地方，一定有这种菌类的痕迹，每次拨开厚厚的叶子发现它，有两种思绪在我心中起伏，它这么可爱，又是一个新生的生命，你怎么舍得下手，但我终究只是凡夫俗子，到底垂涎它的美味，说到这，禁不住唾液的浸润了。 </p><p>许久未写一些无用的文字了，笔墨有些生疏，我的作文本也快一年没有动过了，是时候重新捡起来了。除了计算机，文学艺术确实是我生平另一大爱好，一来我这个俗人喜欢附庸风雅，二来用来打发时间，在物欲横流，利益至上的当今，还有什么比看这些无用书写无用字更舒服的呢，不为无用之事，何以遣有生之涯？</p>]]></content>
    
    
    <summary type="html">下雨了，想起和雨的一点往事~</summary>
    
    
    
    <category term="随笔" scheme="https://buyixiao.github.io/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="雨" scheme="https://buyixiao.github.io/tags/%E9%9B%A8/"/>
    
  </entry>
  
  <entry>
    <title>pandas 合并具有相同结构的 csv</title>
    <link href="https://buyixiao.github.io/blog/merge-csv-with-same-columns.html"/>
    <id>https://buyixiao.github.io/blog/merge-csv-with-same-columns.html</id>
    <published>2022-04-13T08:37:23.000Z</published>
    <updated>2022-04-22T04:23:43.905Z</updated>
    
    <content type="html"><![CDATA[<p>只要某文件夹下所有的 csv 文件结构相同，在文件夹路径运行以下代码就能自动合并，输出结果在 all.csv ，结果 csv 在原有的 csv 结构上新增一列 origin_file_name，值为原来的 csv 文件名，保证了没有信息的衰减。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># author:           inspurer(月小水长)</span></span><br><span class="line"><span class="comment"># create_time:      2022/4/13 10:33</span></span><br><span class="line"><span class="comment"># 运行环境           Python3.6+</span></span><br><span class="line"><span class="comment"># github            https://github.com/inspurer</span></span><br><span class="line"><span class="comment"># website           https://buyixiao.github.io/</span></span><br><span class="line"><span class="comment"># 微信公众号         月小水长</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">result_csv = <span class="string">&#x27;all.csv&#x27;</span></span><br><span class="line">all_cols = []</span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> os.listdir(<span class="string">&#x27;.&#x27;</span>):</span><br><span class="line">    <span class="keyword">if</span> file.endswith(<span class="string">&#x27;.csv&#x27;</span>) <span class="keyword">and</span> <span class="keyword">not</span> file == result_csv:</span><br><span class="line">        df = pd.read_csv(file)</span><br><span class="line">        all_cols = df.columns.values.tolist()</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(all_cols) == <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">raise</span> Exception(<span class="string">&quot;当前目录下没有要合并的 csv 文件&quot;</span>)</span><br><span class="line">all_cols.insert(<span class="number">0</span>, <span class="string">&#x27;origin_file_name&#x27;</span>)</span><br><span class="line">all_df = pd.DataFrame(&#123;col: [] <span class="keyword">for</span> col <span class="keyword">in</span> all_cols&#125;)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> os.listdir(<span class="string">&#x27;.&#x27;</span>):</span><br><span class="line">    <span class="keyword">if</span> file.endswith(<span class="string">&#x27;.csv&#x27;</span>) <span class="keyword">and</span> <span class="keyword">not</span> file == result_csv:</span><br><span class="line">        df = pd.read_csv(file)</span><br><span class="line">        df.insert(<span class="number">0</span>, <span class="string">&#x27;origin_file_name&#x27;</span>, [file <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(df.shape[<span class="number">0</span>])])</span><br><span class="line">        all_df = all_df.append(df, ignore_index=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">all_df.to_csv(result_csv, index=<span class="literal">False</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">不需要指定 csv 有哪些列，把本代码放到具有同一结构的 csv 构成的文件夹中，运行代码就能合并所有 csv 并输出结果到 all.csv</summary>
    
    
    
    <category term="Python" scheme="https://buyixiao.github.io/categories/Python/"/>
    
    
    <category term="pandas" scheme="https://buyixiao.github.io/tags/pandas/"/>
    
    <category term="csv 合并" scheme="https://buyixiao.github.io/tags/csv-%E5%90%88%E5%B9%B6/"/>
    
  </entry>
  
  <entry>
    <title>pandas 分层抽样</title>
    <link href="https://buyixiao.github.io/blog/pandas-stratified-sampling.html"/>
    <id>https://buyixiao.github.io/blog/pandas-stratified-sampling.html</id>
    <published>2022-04-02T15:19:13.000Z</published>
    <updated>2022-04-13T08:54:19.059Z</updated>
    
    <content type="html"><![CDATA[<p>dataframe 里面要有 created_at 一列，格式 %Y-%m-%d %H:%M:%S，首先提取出小时，然后分层（组）抽样，保存到 csv 中，话不多说，上代码～</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># author:           inspurer(月小水长)</span></span><br><span class="line"><span class="comment"># create_time:      2022/4/2 22:58</span></span><br><span class="line"><span class="comment"># 运行环境           Python3.6+</span></span><br><span class="line"><span class="comment"># github            https://github.com/inspurer</span></span><br><span class="line"><span class="comment"># 微信公众号         月小水长</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">input_file = <span class="string">&#x27;RussiaUkraine1.csv&#x27;</span></span><br><span class="line">output_file = <span class="string">&#x27;RussiaUkraine2.csv&#x27;</span></span><br><span class="line"></span><br><span class="line">df = pd.read_csv(input_file)</span><br><span class="line"><span class="comment"># 新增一列 hour</span></span><br><span class="line">df[<span class="string">&#x27;hour&#x27;</span>] = pd.to_datetime(df[<span class="string">&#x27;created_at&#x27;</span>]).dt.hour  <span class="comment"># 时间</span></span><br><span class="line"><span class="comment"># 抽样比例 1%</span></span><br><span class="line">res_df = df.groupby(df[<span class="string">&#x27;hour&#x27;</span>]).apply(<span class="keyword">lambda</span> x: x.sample(frac=<span class="number">0.01</span>))</span><br><span class="line">res_df.to_csv(output_file, index=<span class="literal">False</span>, encoding=<span class="string">&#x27;utf-8-sig&#x27;</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">将 dataframe 按照小时分层（组）后再按照比例随机抽样</summary>
    
    
    
    <category term="Python" scheme="https://buyixiao.github.io/categories/Python/"/>
    
    
    <category term="pandas" scheme="https://buyixiao.github.io/tags/pandas/"/>
    
    <category term="分层抽样" scheme="https://buyixiao.github.io/tags/%E5%88%86%E5%B1%82%E6%8A%BD%E6%A0%B7/"/>
    
  </entry>
  
  <entry>
    <title>ip 访问正常，解析到此 ip 的域名却指向了 nginx 默认首页</title>
    <link href="https://buyixiao.github.io/blog/domainname-to-nginx-default-but-ip-to-ours.html"/>
    <id>https://buyixiao.github.io/blog/domainname-to-nginx-default-but-ip-to-ours.html</id>
    <published>2022-03-10T13:58:38.000Z</published>
    <updated>2022-03-10T14:37:51.860Z</updated>
    
    <content type="html"><![CDATA[<h3 id="背景环境"><a href="#背景环境" class="headerlink" title="背景环境"></a>背景环境</h3><p>硬件：阿里云 2 核 4 G 轻量应用级服务器</p><p>操作系统：Ubuntu 18.04.6 LTS</p><p> nginx version：nginx/1.14.0 (Ubuntu)</p><h3 id="故障表现"><a href="#故障表现" class="headerlink" title="故障表现"></a>故障表现</h3><p>域名成功解析到了 ip，此 ip 访问自己的应用正常，但是域名访问却指向了 nginx 的 index page。</p><h3 id="原因剖析"><a href="#原因剖析" class="headerlink" title="原因剖析"></a>原因剖析</h3><p>首先必须了解该问题涉及到的 nginx 的相关知识。</p><table><thead><tr><th align="center">nginx 相关配置</th><th align="center">路径</th></tr></thead><tbody><tr><td align="center">总配置文件</td><td align="center">/etc/nginx/nginx.conf</td></tr><tr><td align="center">nginx 配置的默认 server 配置文件</td><td align="center">/etc/nginx/sites-enabled/default</td></tr><tr><td align="center">我们自定义的 server 配置文件</td><td align="center">/etc/nginx/conf.d 下所有以 .conf 后缀的文件</td></tr></tbody></table><p>后面两个路径可以在总配置文件中找到：</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">include</span> /etc/nginx/conf.d/<span class="regexp">*.conf</span>;</span><br><span class="line"><span class="attribute">include</span> /etc/nginx/sites-enabled/*;</span><br></pre></td></tr></table></figure><blockquote><p>注意第二个路径是 sites-enabled 而不是 sites-available，这两者区别可自行 google，相关知识和本文问题无关。</p></blockquote><p>nginx 默认的 server 配置文件如下：</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">server</span> &#123;</span><br><span class="line">        <span class="attribute">listen</span> <span class="number">80</span> default_server;</span><br><span class="line">        <span class="attribute">listen</span> [::]:<span class="number">80</span> default_server;</span><br><span class="line"></span><br><span class="line">        <span class="attribute">root</span> /var/www/html;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add index.php to the list if you are using PHP</span></span><br><span class="line">        <span class="attribute">index</span> index.html index.htm index.nginx-debian.html;</span><br><span class="line"></span><br><span class="line">        <span class="attribute">server_name</span> _;</span><br><span class="line"></span><br><span class="line">        <span class="attribute">location</span> / &#123;</span><br><span class="line">                <span class="comment"># First attempt to serve request as file, then</span></span><br><span class="line">                <span class="comment"># as directory, then fall back to displaying a 404.</span></span><br><span class="line">                <span class="attribute">try_files</span> $uri $uri/ =<span class="number">404</span>;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>而当前的 ip 与之对应的自定义 server 配置如下：</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="attribute">upstream</span> django &#123;</span><br><span class="line">    <span class="attribute">server</span> <span class="number">127.0.0.1:9595</span>;  <span class="comment"># uwsgi 配置的ip和端口</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="section">server</span> &#123;</span><br><span class="line">    <span class="attribute">listen</span> <span class="number">80</span>;  <span class="comment"># 监听80端口</span></span><br><span class="line">    <span class="attribute">server_name</span> <span class="number">120.77.233.137</span>;</span><br><span class="line"></span><br><span class="line">    <span class="attribute">location</span> / &#123;</span><br><span class="line">        <span class="comment"># 请求转发到 uwsgi 服务器</span></span><br><span class="line">        <span class="attribute">proxy_pass</span> http://django;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 设置请求头，并将头信息传递给服务器端</span></span><br><span class="line">        <span class="attribute">proxy_set_header</span> Host $host;</span><br><span class="line">        <span class="attribute">proxy_set_header</span> X-Real-IP $remote_addr;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>可以自定义的 server_name 为 120.77.233.137，域名 kcool.top 访问时不会匹配到这个 server，就会交给 nginx 的 default_server 处理，也就到了 nginx 的 index page。</p><p>解决办法已经呼之欲出了。</p><h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><p>最好的解决办法是将域名 kcool.top 也加入我们自定义的 server_name 中，和现有的 ip 以空格分隔，即 :</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">server_name</span> kcool.top <span class="number">120.77.233.137</span></span><br></pre></td></tr></table></figure><p>不得不说很多教程说多个 server_name 以逗号分隔实在是太坑了。</p><p>还有一种办法是将总配置文件的下面这行注释掉：</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">include</span> /etc/nginx/sites-enabled/*;</span><br></pre></td></tr></table></figure><p>讲道理会 404 的，我也不知道为什么可行，有空再琢磨~</p>]]></content>
    
    
    <summary type="html">服务器 ip 访问应用正常，解析到此 ip 的域名指向了 nginx index 而不是自己的应用，本文将剖析原因及提出两种解决办法。</summary>
    
    
    
    <category term="nginx" scheme="https://buyixiao.github.io/categories/nginx/"/>
    
    
    <category term="nginx" scheme="https://buyixiao.github.io/tags/nginx/"/>
    
    <category term="default_server" scheme="https://buyixiao.github.io/tags/default-server/"/>
    
    <category term="域名解析" scheme="https://buyixiao.github.io/tags/%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>一站式微博可视化平台</title>
    <link href="https://buyixiao.github.io/blog/one-stop-weibo-visualization.html"/>
    <id>https://buyixiao.github.io/blog/one-stop-weibo-visualization.html</id>
    <published>2022-02-28T07:34:17.000Z</published>
    <updated>2022-04-21T16:43:32.443Z</updated>
    
    <content type="html"><![CDATA[<h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>抽空写了一个微博可视化的网站，前端基于 sb-admin-2，后端基于 django3，微博相关数据则来源于本站维护的 <a href="https://buyixiao.github.io/blog/weibo-super-spider.html">微博超级爬虫系列</a>。</p><p>网站旨在成为一站式微博可视化分析平台，可以选择按照话题/关键词、位置、用户等维度聚合微博进行总体可视分析，也可以选择某一条微博，对它的转发、评论和点赞数据进行透视分析。以及满足个性化的可视化需求。</p><h3 id="访问地址"><a href="#访问地址" class="headerlink" title="访问地址"></a>访问地址</h3><p>域名访问：<a href="http://weibo.buyixiao.xyz/">http://weibo.buyixiao.xyz</a><br>备用访问：<a href="http://8.142.38.214:9920/">http://8.142.38.214:9920</a></p><h3 id="更新日志"><a href="#更新日志" class="headerlink" title="更新日志"></a>更新日志</h3><h4 id="2022-02-08"><a href="#2022-02-08" class="headerlink" title="2022/02/08"></a>2022/02/08</h4><p>网站上线，只完成了对用户/话题微博的时间分布、地理分布、转评赞榜、情感分析，词云等可视分析，可以直观看到用户/话题的数据表；以及一个简易的评论多级转发可视化。</p><p>不能自动抓取微博话题，需要抓取分析的话题请在文末留言，将定期查看留言话题进行抓取分析，并将话题数据导入网站，可视分析结果可直接在网页下载，下载按钮就在每一个图表的右上角。</p><h4 id="2022-02-28"><a href="#2022-02-28" class="headerlink" title="2022/02/28"></a>2022/02/28</h4><p>新上线接口 <a href="http://weibo.buyixiao.xyz/custom-vis/topics-daily-sentiment-compare-visual/">/custom-vis/topics-daily-sentiment-compare-visual/</a>；读者可以自行上传多个话题爬虫的 csv 文件进行对比情感分析可视化。</p><h4 id="2022-03-03"><a href="#2022-03-03" class="headerlink" title="2022/03/03"></a>2022/03/03</h4><p>新上线 location 栏目；收录北京一众高等院校和鸟巢、奥森、鼓楼、景山公园、三里屯等 50 个地标近 5w 条最新微博。</p><h4 id="2022-03-05"><a href="#2022-03-05" class="headerlink" title="2022/03/05"></a>2022/03/05</h4><p>新上线李文亮先生 <a href="https://weibo.com/1139098205/Is9M7taaY">最后一条微博</a> 的评论 LDA 分析，抓取数万条评论，计算确定最优主题困惑度为 5。分析结果地址：<a href="http://weibo.buyixiao.xyz/comment/liwenliang">/comment/liwenliang</a> 。</p><h4 id="2022-04-22"><a href="#2022-04-22" class="headerlink" title="2022/04/22"></a>2022/04/22</h4><p>新上线接口 <a href="http://weibo.buyixiao.xyz/custom-vis/topic-user-co-occurrence-visual/">/custom-vis/topic-user-co-occurrence-visual/</a> ；读者可自行上传话题 csv 文件提取文本中的相互艾特好友的人物共现网络。</p>]]></content>
    
    
    <summary type="html">一键可视分析用户微博、话题微博，位置微博，以及定制可视化，结果可下载。</summary>
    
    
    
    <category term="可视化" scheme="https://buyixiao.github.io/categories/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    
    <category term="可视化" scheme="https://buyixiao.github.io/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    <category term="echarts" scheme="https://buyixiao.github.io/tags/echarts/"/>
    
  </entry>
  
  <entry>
    <title>开源|微博备份助手</title>
    <link href="https://buyixiao.github.io/blog/weibo-user-backup.html"/>
    <id>https://buyixiao.github.io/blog/weibo-user-backup.html</id>
    <published>2022-02-16T10:30:55.000Z</published>
    <updated>2022-02-16T10:47:35.049Z</updated>
    
    <content type="html"><![CDATA[<h3 id="扩展介绍"><a href="#扩展介绍" class="headerlink" title="扩展介绍"></a>扩展介绍</h3><p>得空写了个微博用户备份助手。</p><p>它的作用是备份用户自己或者任意微博用户的微博数据，并将结果保存到本地的 xlsx 文件。</p><p><img src="https://s2.loli.net/2022/02/16/TKsNObdBpVhultv.png" alt="weibo_backup_show_zip.png"></p><h3 id="使用指南"><a href="#使用指南" class="headerlink" title="使用指南"></a>使用指南</h3><p>1、确保在浏览器登录了 weibo.cn</p><p>2、在 weibo.cn 或者 weibo.com 站点内的微博用户主页上点击扩展图标，会显示如上图，自动解析数字 uid，然后点击开始抓取按钮即可；如果自动解析失败，可手动输入然后抓取；如果自定义微博主页用户的数字 uid 获取方式为：在它的主页上任意一条微博上的用户名右键在新标签页打开，浏览器地址栏就能看到数字 uid 了。</p><p>3、抓取时请勿离开页面或者关闭扩展，抓取结束会自动保存 lxsx，每 增量 200 条也会全部保存一次到 lxsx，因此抓取过程中会有多个 lxsx 文件生成，以最后的文件为准。</p><h3 id="安装地址"><a href="#安装地址" class="headerlink" title="安装地址"></a>安装地址</h3><p>上线了 google 商城，可一在线安装。</p><blockquote><p><a href="https://chrome.google.com/webstore/detail/%E5%BE%AE%E5%8D%9A%E5%A4%87%E4%BB%BD%E5%8A%A9%E6%89%8B/kbgjdcobjobchmhfddlfjnnlaaoiejla?hl=zh-CN">https://chrome.google.com/webstore/detail/%E5%BE%AE%E5%8D%9A%E5%A4%87%E4%BB%BD%E5%8A%A9%E6%89%8B/kbgjdcobjobchmhfddlfjnnlaaoiejla?hl=zh-CN</a></p></blockquote><p>代码开源在 github，也可通过源码安装。</p><blockquote><p><a href="https://github.com/Python3Spiders/WeiboBackupExtension">https://github.com/Python3Spiders/WeiboBackupExtension</a></p></blockquote>]]></content>
    
    
    <summary type="html">一个备份指定微博用户发布微博的浏览器扩展，保存为本地 xlsx</summary>
    
    
    
    <category term="Chrome Extension" scheme="https://buyixiao.github.io/categories/Chrome-Extension/"/>
    
    
    <category term="微博用户爬虫" scheme="https://buyixiao.github.io/tags/%E5%BE%AE%E5%8D%9A%E7%94%A8%E6%88%B7%E7%88%AC%E8%99%AB/"/>
    
    <category term="微博备份" scheme="https://buyixiao.github.io/tags/%E5%BE%AE%E5%8D%9A%E5%A4%87%E4%BB%BD/"/>
    
  </entry>
  
  <entry>
    <title>flask 获取被 nginx 反向代理的客户端真实 ip</title>
    <link href="https://buyixiao.github.io/blog/client-real-ip-proxy-by-nginx.html"/>
    <id>https://buyixiao.github.io/blog/client-real-ip-proxy-by-nginx.html</id>
    <published>2022-02-10T09:27:09.000Z</published>
    <updated>2022-02-10T09:44:54.225Z</updated>
    
    <content type="html"><![CDATA[<p>服务端为了防爬虫或其他用途，需要获取客户端真实 ip，在 flask 中获取客户端 ip 的方法如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip = request.remote_addr <span class="comment">#写在 view 中</span></span><br></pre></td></tr></table></figure><p>但是通过 nginx 反向代理后，获取的 ip 全部变成了 127.0.0.1。</p><p>可以在 nginx 中的配置文件中 location 下块添加一行：</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置请求头，并将头信息传递给服务器端</span></span><br><span class="line"><span class="attribute">proxy_set_header</span> X-Real-IP $remote_addr;</span><br></pre></td></tr></table></figure><p>然后在 flask 中通过以下代码就能获取真实 ip 了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip = request.headers[<span class="string">&#x27;X-Real-IP&#x27;</span>]</span><br></pre></td></tr></table></figure><p>很多类似的教程坑在，只给出一个类似 <code>ip = request.headers[&#39;X-Forwarded-For&#39;] </code>根本不告诉你 headers 的这个 key 是在 nginx 中配置的。</p>]]></content>
    
    
    <summary type="html">获取被 nginx 反向代理 ip 的保姆级教程，知其所以然。</summary>
    
    
    
    <category term="Python" scheme="https://buyixiao.github.io/categories/Python/"/>
    
    
    <category term="nginx" scheme="https://buyixiao.github.io/tags/nginx/"/>
    
    <category term="python" scheme="https://buyixiao.github.io/tags/python/"/>
    
    <category term="flask" scheme="https://buyixiao.github.io/tags/flask/"/>
    
    <category term="ip" scheme="https://buyixiao.github.io/tags/ip/"/>
    
  </entry>
  
  <entry>
    <title>开源|一个微博去广告、屏蔽关键词的扩展</title>
    <link href="https://buyixiao.github.io/blog/weibo-ads-filter-keyword-blocker.html"/>
    <id>https://buyixiao.github.io/blog/weibo-ads-filter-keyword-blocker.html</id>
    <published>2022-01-27T09:48:17.000Z</published>
    <updated>2022-01-27T09:57:52.483Z</updated>
    
    <content type="html"><![CDATA[<h3 id="功能介绍"><a href="#功能介绍" class="headerlink" title="功能介绍"></a>功能介绍</h3><p>针对 weibo.com 这个站点开发了一款名为微博清理大师的插件，其主要功能如下：</p><p>1、永久去广告。</p><p>2、可添加自定义关键词，你刷到的微博和热搜榜将永远看不到这些关键词，除非你手动删除关键词。</p><p>3、可添加微博用户，你再也刷不到 ta 的微博，也可删除。</p><p>4、微博目前是乱序，插件组织微博按照从新到旧的时间线排列，方便阅读。</p><h3 id="操作介绍"><a href="#操作介绍" class="headerlink" title="操作介绍"></a>操作介绍</h3><p>UI 简单，操作方便。</p><p><img src="https://s2.loli.net/2022/01/27/5A6KUd4p1rwtsVQ.png" alt="配置.png"></p><p>点击上方的 1 处可以增删关键词，2 处可以拉黑、解除拉黑用户。</p><p><img src="https://s2.loli.net/2022/01/27/vSQWJzyrKm3kVPF.png" alt="右键菜单.png"></p><p>也可以选中文字右键添加。</p><h3 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a>原理介绍</h3><p>原理很简单，就是拦截 weibo.com 相关接口的请求，并修改响应，去掉广告和屏蔽词，并使之按照时间正序排列，然后就是工程实现了。</p><p>相关接口指的是全部微博、特别关注、最新微博、好友圈和热搜榜相关的接口。</p><p>代码已经全部开源在 github 上，感兴趣的同学可以去看看。</p><blockquote><p><a href="https://github.com/Python3Spiders/WeiboFilterExtension">https://github.com/Python3Spiders/WeiboFilterExtension</a></p></blockquote><h3 id="安装使用"><a href="#安装使用" class="headerlink" title="安装使用"></a>安装使用</h3><p>这个 project 差不多是 2022 年元旦完成的，一直拖到最近才想到上线 Google Chrome 网上应用商店，但是自从 2022 年 1 月 17 起，这个商店就升级了，从技术手段上阻止了很多拦截广告的扩展，这就直接导致了我的插件无法上线 chrome 商店，就算上线了也不会 work。还有一点是，到 2023 年，几乎 chrome 商店里所有拦截广告的扩展都会失效。这波真是 49 年入国军。</p><p>所以目前来看， chrome 使用这个插件并且 work 的话，只能离线安装 crx 文件，crx 文件地址就是上面那个仓库的根目录下的 WeiboFilterExtension.crx 文件。</p><p>下载 crx 文件后，在 chrome 浏览器输入  <code>chrome://extensions/</code> 进入到 chrome 扩展管理页面，然后打开右上方的开发者模式，把这个 crx 文件拖到这个界面即可。</p><p>或者选择源码安装，clone github 地址，选择左上方加载已解压的 扩展程序，选择代码文件夹即可。</p>]]></content>
    
    
    <summary type="html">去广告，屏蔽关键词，拉黑用户，组织乱序微博按照时间线排列等多功能</summary>
    
    
    
    <category term="Chrome Extension" scheme="https://buyixiao.github.io/categories/Chrome-Extension/"/>
    
    
    <category term="微博" scheme="https://buyixiao.github.io/tags/%E5%BE%AE%E5%8D%9A/"/>
    
    <category term="chrome 扩展" scheme="https://buyixiao.github.io/tags/chrome-%E6%89%A9%E5%B1%95/"/>
    
    <category term="去广告" scheme="https://buyixiao.github.io/tags/%E5%8E%BB%E5%B9%BF%E5%91%8A/"/>
    
  </entry>
  
  <entry>
    <title>聚源新闻爬虫及网站介绍</title>
    <link href="https://buyixiao.github.io/blog/all-news-spider.html"/>
    <id>https://buyixiao.github.io/blog/all-news-spider.html</id>
    <published>2022-01-24T11:18:24.000Z</published>
    <updated>2022-05-31T03:59:26.619Z</updated>
    
    <content type="html"><![CDATA[<h3 id="项目介绍"><a href="#项目介绍" class="headerlink" title="项目介绍"></a>项目介绍</h3><div class="note primary simple"><p>针对泰晤士报，纽约时报，BBC News 等国外主流媒体进行关键词抓取，针对澎湃新闻，新浪新闻，腾讯新闻等国内主流媒体进行分类抓取。</p></div><div class="note primary simple"><p>短期目前旨在爬取所有新闻门户网站的新闻，每个门户网站爬虫开箱即用，并自动保存到同目录下的 csv/excel 文件中。</p></div><div class="note primary simple"><p>长期目标是打造一个信息流聚合平台，或者进行更高层面的比如社会舆情、新闻地理可视化等的处理。</p></div><h3 id="github-地址"><a href="#github-地址" class="headerlink" title="github 地址"></a>github 地址</h3><p>具体使用可以参考 github 上的 demo 和 wiki：</p><p><a href="https://github.com/Python3Spiders/AllNewsSpider">https://github.com/Python3Spiders/AllNewsSpider</a></p><h3 id="网站地址"><a href="#网站地址" class="headerlink" title="网站地址"></a>网站地址</h3><p>新闻数据展示网站：</p><p><a href="http://buyixiao.xyz/">http://buyixiao.xyz/</a></p><p>上面失效了话，使用如下备用地址访问：</p><p><a href="http://8.142.38.214/">http://8.142.38.214/</a></p><p><img src="https://s2.loli.net/2022/01/22/ofuOjMwS9Q6G43V.png" alt="聚源新闻网站_zip.png"></p><p><font size=4 color="red">服务器性能有限，新开了其他网站，暂时关闭此网站，2022/05/31 记录。</font></p><h3 id="项目赞助"><a href="#项目赞助" class="headerlink" title="项目赞助"></a>项目赞助</h3><p>博主维护着不少的开源项目，见于 <a href="https://github.com/inspurer">https://github.com/inspurer</a> ，耗费着大量的时间和精力，如果项目帮助到了你，可以点击下方赞赏，助力项目长期发展。</p>]]></content>
    
    
    <summary type="html">博主开发的一个抓取 BBC News、纽约时报 nytimes、泰晤士报 thetimes、澎湃、腾讯等新闻网站的聚合爬虫，以及聚合新闻网站介绍</summary>
    
    
    
    <category term="爬虫" scheme="https://buyixiao.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
    <category term="bbcnews" scheme="https://buyixiao.github.io/tags/bbcnews/"/>
    
    <category term="nytimes" scheme="https://buyixiao.github.io/tags/nytimes/"/>
    
    <category term="thetimes" scheme="https://buyixiao.github.io/tags/thetimes/"/>
    
    <category term="pengpai" scheme="https://buyixiao.github.io/tags/pengpai/"/>
    
    <category term="sina" scheme="https://buyixiao.github.io/tags/sina/"/>
    
    <category term="tencent" scheme="https://buyixiao.github.io/tags/tencent/"/>
    
  </entry>
  
  <entry>
    <title>罗素：我为什么而活</title>
    <link href="https://buyixiao.github.io/blog/what-i-have-lived-for-by-russell.html"/>
    <id>https://buyixiao.github.io/blog/what-i-have-lived-for-by-russell.html</id>
    <published>2022-01-22T13:45:53.000Z</published>
    <updated>2022-01-22T00:35:05.096Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>全文摘自《我为什么而活着》，《我为什么而活着》是《罗素自传》的序言，作者是伯特兰·罗素。</p></blockquote><h2 id="中文译文"><a href="#中文译文" class="headerlink" title="中文译文"></a>中文译文</h2><p>对爱情的渴望，对知识的追求，对人类苦难不可遏制的同情心，这三种纯洁而无比强烈的激情支配着我的一生。这三种激情，就像飓风一样，在深深的苦海上，肆意地把我吹来吹去，吹到濒临绝望的边缘。</p><p>我寻求爱情，首先因为爱情给我带来狂喜，它如此强烈以致我经常愿意为了几小时的欢愉而牺牲生命中的其他一切。我寻求爱情，其次是因为爱情可以解除孤寂一—那是一颗震颤的心，在世界的边缘，俯瞰那冰冷死寂、深不可测的深渊。我寻求爱情，最后是因为在爱情的结合中，我看到圣徒和诗人们所想像的天堂景象的神秘缩影。这就是我所寻求的，虽然它对人生似乎过于美好，然而最终我还是得到了它。</p><p>我以同样的热情寻求知识，我渴望了解人的心灵。我渴望知道星星为什么闪闪发光，我试图理解毕达哥拉斯的思想威力，即数字支配着万物流转。这方面我获得一些成就，然而并不多。</p><p>爱情和知识，尽其可能地把我引上天堂，但是同情心总把我带回尘世。痛苦的呼唤经常在我心中回荡，饥饿的儿童，被压迫被折磨者，被儿女视为负担的无助的老人以及充满孤寂、贫穷和痛苦的整个世界，都是对人类应有生活的嘲讽。我渴望减轻这些不幸，但是我无能为力，而且我自己也深受其害。</p><p>这就是我的一生，我觉得值得为它活着。如果有机会的话，我还乐意再活一次。</p><h2 id="英文原文"><a href="#英文原文" class="headerlink" title="英文原文"></a>英文原文</h2><p>《What I Have Lived For》 by Bertrand Russell</p><p>Three passions, simple but overwhelmingly strong, have governed my life: the longing for love, the search for knowledge, and unbearable pity for the suffering of mankind. These passions, like great winds, have blown me hither and thither, in a wayward course, over a great ocean of anguish, reaching to the very verge of despair.</p><p>I have sought love, first, because it brings ecstasy - ecstasy so great that I would often have sacrificed all the rest of life for a few hours of this joy. I have sought it, next, because it relieves loneliness–that terrible loneliness in which one shivering consciousness looks over the rim of the world into the cold unfathomable lifeless abyss. I have sought it finally, because in the union of love I have seen, in a mystic miniature, the prefiguring vision of the heaven that saints and poets have imagined. This is what I sought, and though it might seem too good for human life, this is what–at last–I have found.</p><p>With equal passion I have sought knowledge. I have wished to understand the hearts of men. I have wished to know why the stars shine. And I have tried to apprehend the Pythagorean power by which number holds sway above the flux. A little of this, but not much, I have achieved.</p><p>Love and knowledge, so far as they were possible, led upward toward the heavens. But always pity brought me back to earth. Echoes of cries of pain reverberate in my heart. Children in famine, victims tortured by oppressors, helpless old people a burden to their sons, and the whole world of loneliness, poverty, and pain make a mockery of what human life should be. I long to alleviate this evil, but I cannot, and I too suffer.</p><p>This has been my life. I have found it worth living, and would gladly live it again if the chance were offered me.</p>]]></content>
    
    
    <summary type="html">对爱情的渴望，对知识的追求，对人类苦难不可遏制的同情心，这三种纯洁而无比强烈的激情支配着我的一生。</summary>
    
    
    
    <category term="励志" scheme="https://buyixiao.github.io/categories/%E5%8A%B1%E5%BF%97/"/>
    
    
    <category term="罗素" scheme="https://buyixiao.github.io/tags/%E7%BD%97%E7%B4%A0/"/>
    
  </entry>
  
  <entry>
    <title>李开复：追随我心</title>
    <link href="https://buyixiao.github.io/blog/follow-your-heart-by-likaifu.html"/>
    <id>https://buyixiao.github.io/blog/follow-your-heart-by-likaifu.html</id>
    <published>2022-01-21T06:48:38.000Z</published>
    <updated>2022-01-21T06:53:05.304Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>全文摘自李开复相关传记，《追随我心》，作者不详。</p></blockquote><p>并不很久的以前，也就在 1979 年到 1980 年间，在哥伦比亚大学，两个政治科学系大一的新生，在课堂上总是没精打采。其中一个是来自台湾的华裔，喜欢窝在教室左后方的一隅，听得无趣，索性呼呼大睡。这个男孩叫李开复，此君并非厌学，而是对政治科学越来越不感兴趣。蹉跎到大二下学期，他终于决定快刀斩乱麻——转系，改学自己感兴趣的计算机。</p><p>兴趣是什么？兴趣就意味着天赋。李开复在计算机系如鱼得水，左右逢源，两年后毕业，成绩居全系之首。这样的学生用不着按部就班。在教授的推荐下，李开复进入在计算机领域独领风骚的卡内基•梅隆大学，直接攻读博士。计算机学院的院长找他谈话，劈头就问：“读博士的目的是什么？”李开复大声答：“我从大学带走的将是一篇改变世界的、顶尖的博士论文。”院长予以纠正，说：“你从这儿带走的最有价值的东西，不是一篇论文，而是你分析、思考的能力，研究、发现真理的经验，以及科学家的胸怀。这样，当你有一天改变研究方向，依然可以在任何一个新的领域出类拔萃。”李开复选定语音识别为攻读方向，经过一年“热恋”，他发现专家系统其冷如冰，远不如统计学有情有义。李开复决心“移情别恋”。他担心导师发怒，谁知得到的回答竟是：“开复，你对专家系统和统计的观点，我是不赞同的，但我可以支持你用统计的方法去做，因为我相信科学没有绝对的对错，我们都是平等的。而且，我更相信一个富有激情的人可以找到更好的解决方案。”李开复从导师的大度悟到科学的真谛，他全力以赴，放手一搏。３年过去了，李开复的研究成果及博士论文，引发了那年语音世界最大的冲击波。26 岁的李开复功成名就，成为卡内基•梅隆大学最年轻的副教授。天之骄子，有尊严，有地位，有课题，有经费，出任大公司顾问，飞赴各地讲学，包括去他的祖籍之邦、魂之所系的祖国大陆。</p><p>“让世界因你而不同！”这是李开复埋在心底多年的梦想。1990 年，苹果公司的一个邀请电话让李开复开始审视自己：“开复，你是想一辈子写一堆像废纸一样的学术论文，还是想真正地改变世界？”面对苹果公司的召唤，李开复旋即做出回应，走出象牙塔，加盟“改变世界”的大军。在苹果公司，李开复感受到了从纸上谈兵转入实战的无穷乐趣。1995 年，33 岁的李开复出任苹果公司的副总裁。</p><p>但是他仍然不满足，依然要跳槽，因为硅谷的另一家公司 SGI 发出了更有诱惑力的邀请——“你想做什么，然后我们根据你的兴趣对公司进行改组。”不是他们缺什么人才，让你去填补，而是诚恳地询问你需要什么平台，以便为你量身搭建。这样的机遇，李开复岂能错过！双方一拍即合，　1996 年７月，李开复跳槽去了 SGI 。李开复奉行“自己设计自己”的人生信条，怎奈 SGI 是一家硬件公司，开复的长处却在软件开发，这就等于在篮球场上跑马，任是赤兔、骅骝，也撒不开四蹄。日复一日，李开复萌生去意。对于下一个选择，他立下两条标准：一是做软件，二是去中国。</p><p>机会来了。其实机会无处不在，就看你有没有做好准备。彼时，比尔•盖茨创立的微软王国要把触角伸向中国，李开复成为它的不二人选。时间：1998 年金秋；职务：微软中国研究院院长。李开复在中国市场的开拓，值得写部书来描述，那是一种完全不同的创新理念、绝对领先的科学技术在神州大地生根发芽。微软只是起用了一个人，就开拓了中国市场；李开复只是“追随我心”，就一跃成为微软王国的副总裁。在你我想来，这该是李开复的最后一站。在微软占据高位，与比尔•盖茨亲密共事，坐拥财富和风光，“花迎喜气皆知笑，鸟识欢心亦解歌”。人生至此，夫复何求？李开复不这么想，他后来回忆：“我如同一部庞大机器上的零件，在中规中矩、没有任何发挥空间的环境下运行着。这是一个随时随地都可以被替换的光鲜零件。那种价值的缺失感以及精神上的落寞占据了我的内心。”微软既然已无成长空间，那就走吧！到哪儿去？他相中了 Google。但他清醒地意识到，管理更多的人马，不是自己的所爱，他渴望从无到有的创新，而不是经营一个巨无霸。于是，在2009 年９月，李开复又一次选择潇洒地离去。向总部递交辞呈之际，Google 高管艾伦•尤斯塔斯试图用更优厚的条件予以挽留。李开复真诚地说：“我的人生还有一个缺憾没有实现，现在得去弥补。我可能创办一家‘创新工场’，和中国青年一起创造新的技术奇迹。”</p><p>如今，李开复正在按照他本人的意愿，在神州大地进行“创新工场”试验。他会成功吗？我想这是毫无疑问的，也是次要又次要的，那么，最主要的一点是什么呢？诚如他自己所言：“人生在世时间非常短，如果你总是不敢做想做的事情，那么一生过去了，你留下来的只有悔恨，只有懊恼。”“我步入丛林，因为我希望生活得有意义，我希望活得深刻，并汲取生命中所有的精华，然后从中学习，以免让我在生命终结时，才发现自己从来没有活过。”</p>]]></content>
    
    
    <summary type="html">我步入丛林,因为我希望生活得有意义,我希望活得深刻,并汲取生命中所有的精华,然后从中学习,以免让我在生命终结时,才发现自己从来没有活过。</summary>
    
    
    
    <category term="励志" scheme="https://buyixiao.github.io/categories/%E5%8A%B1%E5%BF%97/"/>
    
    
    <category term="李开复" scheme="https://buyixiao.github.io/tags/%E6%9D%8E%E5%BC%80%E5%A4%8D/"/>
    
  </entry>
  
  <entry>
    <title>新微博通知 chrome 扩展使用介绍</title>
    <link href="https://buyixiao.github.io/blog/new-weibo-notify-chrome-extension.html"/>
    <id>https://buyixiao.github.io/blog/new-weibo-notify-chrome-extension.html</id>
    <published>2022-01-20T12:03:48.000Z</published>
    <updated>2022-01-20T12:35:04.626Z</updated>
    
    <content type="html"><![CDATA[<h2 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h2><p>博主开发的新微博通知助手，已上架谷歌扩展商城：<a href="https://chrome.google.com/webstore/detail/%E5%BE%AE%E5%8D%9A%E9%80%9A%E7%9F%A5%E5%8A%A9%E6%89%8B/cpmlmjdimlnhgnakcjfmbmfglhkaoago?hl=zh-CN">点击前往商城免费安装</a>。</p><p>它的作用是接收指定微博用户的最新微博通知（不包括置顶微博）。</p><p>它的特色是不需要 Cookie，不需要登录无状态即可收到桌面通知。</p><p>它的操作也特别简单，自动解析 uid，点击保存即可。</p><p><img src="https://s2.loli.net/2022/01/20/ygVbRSAwGJXvO7Y.png" alt="chrome_extension_zip.png"></p><p>然后插件就会定时 20s 去轮询这个人的微博状态，一有它的最新微博就会有系统级的桌面通知。</p><h2 id="设置"><a href="#设置" class="headerlink" title="设置"></a>设置</h2><p>1、注意解析的地址栏，必须是微博数字 uid，微博用户自定义域名的 id 不行。比如谢娜的微博主页自定义成 xiena，但是每一个微博用户都有数字 uid 的，怎么找呢，秘诀就是在这个微博用户任意一条的微博的用户名上右键在新 tab 打开链接，然后地址栏就有数字 uid 了。</p><p>这样自动解析成功，点击保存就能接受新微博通知了。</p><p>无论自动解析成否，也可以手动输入数字 uid；可以在上图 2 处输入框输入，也可以在上图 3 处 add uid 输入。保存的 uid 实时显示在 3 处浅绿色标签，一个 uid 对应一个标签。可以点击标签上的 X 删除 uid。</p><p>2、明明保存了配置，显示添加成功，也有新微博了，就是收不到通知？</p><p>可能在电脑的设置里关闭了 Chrome 的桌面通知权限？打开即可。</p><p>浏览器在后台或前台运行的话，能实时通知，如果关闭了，下次打开也能收到最新通知。</p><p>如果没网络那就肯定收不到通知了。</p><p>3、轮询时间不可设置</p><p>轮询时间也不能设置，固定 20s。何哉？因为本插件的定位是非常克制的，没有 cookie，登录，无状态。如果想同时接收很多人的通知，建议直接在浏览器打开 weibo.com。本插件的最佳食用方式是少量的 uid，uid 对应的博主不频繁发微博这种。</p>]]></content>
    
    
    <summary type="html">不登录，零状态，低配置，博主开发的一个新微博通知的 chrome 扩展</summary>
    
    
    
    <category term="Chrome Extension" scheme="https://buyixiao.github.io/categories/Chrome-Extension/"/>
    
    
    <category term="微博" scheme="https://buyixiao.github.io/tags/%E5%BE%AE%E5%8D%9A/"/>
    
    <category term="chrome 扩展" scheme="https://buyixiao.github.io/tags/chrome-%E6%89%A9%E5%B1%95/"/>
    
  </entry>
  
  <entry>
    <title>浅谈微博评论水军和异常流量</title>
    <link href="https://buyixiao.github.io/blog/weibo-comment-robot-analysis.html"/>
    <id>https://buyixiao.github.io/blog/weibo-comment-robot-analysis.html</id>
    <published>2022-01-20T11:37:07.000Z</published>
    <updated>2022-01-22T10:47:13.423Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>近年来，微博评论区的异常评论流量现象甚嚣尘上，背后是大量的营销账号的扰乱视听以及作为他们的傀儡的水军账号的推波助澜，本篇利用微博评论爬虫采集的公开数据，简单分析了这些现象的一些表征和原因。</p><h3 id="数说"><a href="#数说" class="headerlink" title="数说"></a>数说</h3><p>以人民日报发表的关于 <strong>#吴亦凡被批捕#</strong> 这条微博及其评论数据为例子。</p><p><img src="https://s2.loli.net/2022/01/20/AXRmgYvsIUJOHdb.png" alt="人民日报.png"></p><p>网页显示有近 18w 条微博，实际抓取去重后有 10w 稍有余的数据，包括根评论和回复，后文分析评论时，仅针对分析发博一天内的评论。抓取保存的评论字段信息如下</p><table><thead><tr><th align="center">字段名</th><th align="center">含义</th></tr></thead><tbody><tr><td align="center">parent_cid</td><td align="center">该回复所属的根评论 id，只有回复评论有值，根评论为空</td></tr><tr><td align="center">cid</td><td align="center">评论 id</td></tr><tr><td align="center">time</td><td align="center">评论发表时间</td></tr><tr><td align="center">text</td><td align="center">评论内容</td></tr><tr><td align="center">like_count</td><td align="center">评论点赞数</td></tr><tr><td align="center">reply_count</td><td align="center">该根评论有多少条回复评论，只有根评论有值，回复评论为0</td></tr><tr><td align="center">uid</td><td align="center">评论者 id</td></tr><tr><td align="center">username</td><td align="center">评论者用户名</td></tr><tr><td align="center">following</td><td align="center">评论者关注数</td></tr><tr><td align="center">followed</td><td align="center">评论者粉丝数</td></tr><tr><td align="center">gender</td><td align="center">评论者性别</td></tr></tbody></table><p>第一步，可视化该条微博发布后一天内每分钟新发评论数量时间线。</p><p><img src="https://s2.loli.net/2022/01/20/dIz632LbKUP7e1C.png" alt="发博后每分钟评论数.png"></p><p>每分钟评论数在短时间内指数型急剧上升，最后又以一象限双曲线形式下降，符合常理认知。同时可以看出，在发博时间 2021/08/16 20:30 过去 840mins，也就是发博 16 小时后，2021/08/17 10:30 时有个极大值，why？迫于本篇推送选题的压力，我马上想到了可能是水军账号这个时候营业了，但是我分析了这个时间段发布评论的用户，肉眼可见几乎没有水军账号。于是乎，我翻开了微博的历史热搜数据，发现在这个时间点，#都美竹感谢朝阳公安和粉丝# 这个话题冲到了热搜第一，很显然，是由于该关联话题的热度扩散到了这条微博。</p><p><img src="https://s2.loli.net/2022/01/20/SYlnov7ZT8JjrMO.png" alt="历史热搜.png"></p><p>如果查证历史热搜数据该时间点无相关热搜，且几乎没有观察到该时间点附近评论营销水军内容，那么下降曲线就会是完美的一象限双曲线；否则就需要确定是相关热搜或者是营销水军，亦或者是它们共同作用的结果。</p><p>第二步，怎么大致判断评论中水军账号呢，我的做法是 group_by uid。</p><p>分析结果显示，一天之内，一个账号最多针对该微博发布了 26 条评论，发布 10 条评论以上的账号多达 30 余人，这些账号具有一定的营销号或水军嫌疑，目前只能手动点开微博主页浏览去确定，长期地，我想输出一个模型，根据 uid 判断账号是否是营销号或者水军账号，目前的想法就是根据它的发博连续性，关注粉丝之比，账号新旧程序等维度考量，大家有好想法欢迎留言。</p><p>最后可视化每分钟评论的平均文本长度如下。</p><p><img src="https://s2.loli.net/2022/01/20/gvCaqyJENzZriuj.png" alt="发博后每分钟评论平均长度.png"></p><p>处理时去除了 html 标签表情等非文本内容，但是上图依旧有很大的锯齿，应该用中值滤波处理之，不过走势应该不会变。</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>1、微博历史热搜数据：<a href="https://www.weibotop.cn/2.0/">https://www.weibotop.cn/2.0/</a></p><p>2、研究报告 | 微博评论中的水军异常流量分析：<a href="https://zhuanlan.zhihu.com/p/436967668">https://zhuanlan.zhihu.com/p/436967668</a></p>]]></content>
    
    
    <summary type="html">用数据可视化得出对于微博评论水军流量的一些粗鄙之见</summary>
    
    
    
    <category term="可视化" scheme="https://buyixiao.github.io/categories/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    
    <category term="微博评论" scheme="https://buyixiao.github.io/tags/%E5%BE%AE%E5%8D%9A%E8%AF%84%E8%AE%BA/"/>
    
    <category term="可视化" scheme="https://buyixiao.github.io/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    <category term="水军" scheme="https://buyixiao.github.io/tags/%E6%B0%B4%E5%86%9B/"/>
    
    <category term="营销号" scheme="https://buyixiao.github.io/tags/%E8%90%A5%E9%94%80%E5%8F%B7/"/>
    
  </entry>
  
</feed>
